---
layout: post
title: IOS 音频处理(一)
categories: [ios]
tags: [ios]
description: ios 音频处理
---

iOS提供了音频处理插件，支持混合，均衡，格式转换和实时 输入/输出录制，回放，离线渲染和现场的谈话如VoIP（语音 互联网协议）。您可以动态加载和使用，也就是说，主机，这些强大而灵活的插件， 
在您的iOS应用程序,被称为音频单元，
		音频单元通常不会将其在一个封闭的对象的上下文中工作称为音频处理图形，如 在该图中示出。在这个例子中，你的应用程序的一个方式将音频发送到所述第一音频单元在图中 以上的回调函数，并行使单独控制每一个音频单元。在I / O输出 单元，在此最后的音频单元或任何音频处理直接图形，连接到输出硬件。
<h1>概览</h1>
由于音频单元构成的最低编程层中的IOS音频堆栈，有效地使用它们 
需要更深入的了解比你需要的其他iOS音频技术。除非你需要实时播放合成声音，低延迟的I\O输入输出），或特定的音频单元的特点，先看看在媒体播放器，影音基金会，OpenAL的，或者音频工具箱框架。这些更高级的技术使用音频单元以您的名义，并提供重要的附加功能，如多媒体描述编程指南
<h1>Audio Units提供快速，模块化的音频处理</h1>
<h2>直接使用音频单元的两个最大的优点是：</h2>
<p>1、出色的响应能力。因为你有一个音频单元访问实时优先级的线程渲染、回调函数，音频代码是尽可能接近的金属。合成乐器并实时同步语音的I/O。</p>
<p>2、动态重新配置。音频处理图形API，围绕AUGraph不透明类型内置，让你动态组合，重新配置和重新排列复杂的音频处理链中的一个线程安全的。这样，所有在处理音频。这是iOS中唯一的音频API，提供这种能力</p>
<h1>audio unit’s生命周期</h1>
<p>1，在运行时，得到的引用动态链接库，它定义你想要的音频单元来使用。 </p>
<p>2，实例化的音频单元。</p>
<p>3，配置音频单元所需的类型和以适应你的应用程序的意图。 </p>
<p>4，初始化音频单元到它准备来处理音频。 </p>
<p>5，启动音频流。 </p>
<p>6，控制音频单元。 </p>
<p>7，完成后，释放了音频单元。</p>
<p>音频单元提供了非常有用的个别功能，如立体声声像，混音，音量控制和 音频电平表。托管音频单元，您可以添加这样的功能，你的应用程序。为了获得这些好处， 但是，你必须获得设备与一组基本概念，包括音频数据流格式， 渲染的回调函数，而音频单元的体系结构。</p>
<h1>选择设计模式和构建App</h1>
<p>如何配置I / O单元。 I / O单元有两个独立的元素，一个接受来自音频 输入硬件，一个发送音频输出硬件。</p>
每一个设计模式指示哪些元素或元素，应启用。 
<p>1、其中，所述音频处理图形内，你必须指定音频数据流格式。您必须正确 指定格式，支持音频流。 </p>
<p>2、凡建立音频单元连接且附上您呈现回调函数。音频单元连接是一个正式的结构，它传播的流格式从一个音频单元的输出 
到另一个音频单元的一个输入端。 渲染回调让你感觉音频转换成图表或音操作 在图中的各个样本的水平。</p>

无论您选择哪种设计模式，构建一个音频单元托管在应用程序基本上是相同： 
<p>1，配置你的应用程序的音频会议，以确保您的应用程序正常工作，在系统的上下文和设备硬件。 </p>
<p>2，构造一个音频处理图形。这个多步骤的过程是利用你学到的一切 “音频单元托管基础”（第11页）。</p> 
<p>3，提供一个用户界面来控制图形的音频单元</p>
熟悉这些步骤，这样你就可以将它们应用到自己的项目
<h1>充分利用各音频单元</h1>
大多数文档教你所有的iOS音频单元共享的重要，共同的属性。这些属性包括，例如，需要对你的应用程序的指定，并加载在音频单元在运行时，然后 正确地指定其音频流格式。 
同时，每一个音频单元具有某些独特的特性和要求，从正确的 使用音频采样数据类型，所需配置正确的行为。了解使用详情 每个音频单元的特定能力，让你知道，例如，当使用3D龙头股和 
时而是使用多通道混音器。

<h1>前提</h1>
在阅读此文档之前，建议阅读“一些关于数字音频和线性 PCM“的核心音频概述。此外，检查核心音频词汇表术语，你可能还不熟悉。 要检查您的音频需求可能由更高级别的技术来满足，回顾“使用音频”多媒体 
编程指南。
<h1>如何使用本文档</h1>
如果你喜欢一开始就有一个动手的介绍主办的iOS音频单元，下载示例之一 在iOS的开发中心提供的应用程序，如调音台（MixerHost）或IOHost。回到本文 
回答问题，你可能已经和了解更多信息。如果你想开始你的项目之前扎实的概念基础，阅读“音频单元主机 基本原理“（第11页）第一位。这些API的背后本章explainsthe概念。继续以“建设 
音频单元应用程序“（第29页），以了解采摘设计模式为你的项目和工作流程 构建您的应用程序
如果您有音频单元的一些经验，只是想具体给定类型，你可以开始 “使用特定的音频单元”（第41页）。

<h1>另请参阅</h1>
<p>构建托管的应用程序的音频单元必不可少的参考文档包括以下内容： </p>
<p>■Audio Unit Properties ,参考介绍了可用于配置每种类型的音频单元的属性。 </p>
<p>■Audio Unit Parameters,参考说明你可以用它来控制各类型的音频单元的参数。 </p>
<p>■Audio Unit Component Services ，用于访问音频设备参数和属性，并且描述了各种音频单元的回调函数。 </p>
<p>■Audio Component Services,用于访问音频设备在运行和 音频管理单元实例。 </p>
<p>■Audio Unit Processing Graph Services,参考描述了API，用于构建和操作声音 处理图形，动态可重构的音频处理链。</p> 
<p>■ Core AudioData Types ,参考描述了你所需要的数据结构和类型的主机的音频单元。</p>

<h1>Audio Unit Hosting基础知识</h1>
所有的音频技术在iOS的内置音频单元的顶部，如图1-1所示。较高级别的技术显示在这里，媒体播放器，影音基金会的OpenAL和音频工具箱包装音频单元为特定任务的专用和简化的API.
在你的项目中直接使用的音频设备是唯一正确的选择，当你需要非常高的程度 控制，性能或灵活性，或者当你需要的特定功能（如声学回声取消acoustic echo）可通过直接使用一个音频单元。对于iOS的音频API的就当了概述，并指导 用每一个，请参阅多媒体编程指南
<h1>IOS Audio Units</h1>
<p>iOS provides seven audio units</p>
{% highlight ruby %}
<p>----------------------------------------------------------------</p>
<p>Effect      |		iPod Equalizer</p>
<p>----------------------------------------------------------------</p>
<p>mixing		|		3D Mixer</p>
<p>			|		Multichannel Mixer</p>
<p>----------------------------------------------------------------</p>
<p>I/O         |        Remote I/O</p>
 <p>           |        Voice-Processing I/O</p>
  <p>          |        Generic Output</p>
<p>----------------------------------------------------------------</p>
<p>Format conversion | Format Converter</p>
<p>----------------------------------------------------------------</p>
{% endhighlight %}

 Effect Unit

 --------------------------------------------------------------------------------------------------------------------------------------

iOS4提供1个效果器，iPod的均衡器，使用内置的iPod应用程序相同的均衡器。要查看 这个音频单元的iPod应用程序的用户界面，进入设置> iPod的>均衡器。
当使用这个音频单元，则必须提供自己的UI。这个音频单元提供了一套预先设定的均衡曲线，如低音增强器， POP。

Mixer Units
--------------------------------------------------------------------------------------------------------------------------------------
iOS提供了two mixer units。
<p>1、3D Mixer unit是在其OpenAL的建立奠定了基础。在大多数情况下， 如果你需要的3D Mixer unit的特点，你最好的选择是使用OpenAL的，它提供了更高级别的API，非常适合游戏应用程序。对于示例代码，演示了如何使用OpenAL的，看样品 代码项目oalTouch。对于示例代码，
演示了如何直接使用3D Mixer unit，看项目 Mixer3DHost。</p>
<p>2、Multichannel Mixer unit提供混合任意数量的单声道或立体声数据流，具有立体声 
输出。您可以将每个输入或关闭，设置它的输入增益，并设置它的立体声声像位置。对于 演示如何使用这个音频单元，见示例代码项目（MixerHost）。</p>

I/O Units
--------------------------------------------------------------------------------------------------------------------------------------
iOS提供了三个I / O单元。
Remote I/O unit是最常用的。它连接到输入和输出音频硬件并为您提供低延迟访问单个输入和输出的音频采样值。 它提供了格式转换的硬件音频格式和应用程序的音频格式之间进行，这样做所以由包括格式转换器单元的方式。对于示例代码，演示了如何使用Remote I/O unit，见示例代码项目IOHost和aurioTouch。

Voice-Processing I/O unit 在Remote I/O unit 上加入回声取消。它还提供了自动增益校正，调整语音处理 质量，和屏蔽。

Generic Output unit没有连接到音频硬件，而是提供了一种机制，用于发送处理链的输出到您的应用程序。您通常会使用Generic Output unit做离线音频处理。


<h1>Format Converter Unit</h1>
<p>--------------------------------------------------------------------------------------------------------------------------------------</p>iOS4提供了一种格式转换器单元，它通常用于间接的I/ Ounit的方式。

<h1>Use the Two Audio Unit APIs in Concer</h1>
ios 有一个API直接对audio units工作，另一种用于处理audio Processing graphs.演唱会同时使用这两个api来工作
	1、对于直接使用audio units来工作--配置和控制他们 ----参考Audio Unit Component Services
	2、创建和配置audio processing graph-----参考Audio Unit Processing Graph Services
这两个API之间有一些重叠，根据您的编程风格，你可以自由地混合和匹配。audio unit API 和audio processing graph API都提供功能：
<p>1、获取引用定义的audio units 的动态链接库</p>
<p>2、Instantiating audio units</p>
<p>3、相互连接的audio units和附加render callback</p>
<p>4、Starting and stopping audio flow</p>

本文档提供了代码示例使用这两个API，但侧重于audio processing graph API。 推荐使用audio processing graph AP。您的代码将更加紧凑，易于阅读，而且更适合于支持 动态重新配置（参见“Audio Processing Graphs Provide Thread Safety”（第20页））


<h1>使用指定Identifiers和获得 Audio Units</h1>
----------------------------------------------------------------------------------------------------------------------------------------
要找到一个 audio unit在运行时，在音频组件描述数据结构中设置type, subtype, and manufacturer keys 。你这样做是否使用音频设备或音频处理图 
API。清单1-1显示了如何

Listing 1-1 Creating an audio component description to identify an audio unit

{% highlight ruby %}
AudioComponentDescription ioUnitDescription;

ioUnitDescription.componentType = kAudioUnitType_Output;

ioUnitDescription.componentSubType = kAudioUnitSubType_RemoteIO;

ioUnitDescription.componentManufacturer = kAudioUnitManufacturer_Apple;

ioUnitDescription.componentFlags = 0;

ioUnitDescription.componentFlagsMask = 0;
{% endhighlight %}


Remote I/O unit的配置，注意，这是固定的

ioUnitDescription.componentManufacturer = kAudioUnitManufacturer_Apple;

（The keys for this and other iOS audio units are listed in “Identifier Keys for Audio Units” (page 46).)

要创建一个通用的描述，设置一个或多个 type/subtype 字段 为 0.例：匹配所有的I/O units,将

ioUnitDescription.componentSubType ＝ 0；

通过这两个API，你可以从audio unit（或设置audio units）中获取引用库 。The audio unit API is shown in Listing 1-2.

Listing 1-2 Obtaining an audio unit instance using the audio unit API
{% highlight ruby %}
 AudioComponent foundIoUnitReference = AudioComponentFindNext (

 															NULL,

 															&ioUnitDescription

 															);


AudioUnit ioUnitInstance;

AudioComponentInstanceNew (

 foundIoUnitReference,

 &ioUnitInstance

);
{% endhighlight %}

这里传递NULL是让系统找到第一个系统audio unit匹配的描述（感觉就是让系统自己找到与描述相符合的audio unit），使用的是系统定义的排序。

如果你不是通过以前发现 在该参数中的音频单元的参考，则该函数查找下一个音频单元的匹配的描述。例如，获得引用的所有I / O单元中通过反复调用 

AudioComponentFindNext

第二个参数为AudioComponentFindNext指的是定义audio unit description 的音频单元说明清单1-1（第13页）。

AudioComponentFindNext返回的结果是引用动态链接库来定义audio unit.通过AudioComponentInstanceNew来实例化audio unit

<h3>你可以使用audio procession graph api来实例化audio unit</h3>

Listing 1-3 使用 audio processing graph API实例化audion unit
{% highlight ruby %}
// 申明 和 实例化 audio processing graph
AUGraph processingGraph;

NewAUGraph (&processingGraph);

// 添加audio unit node 到 graph, then instantiate the audio unit

AUNode ioNode;

AUGraphAddNode (

 processingGraph,

 &ioUnitDescription,

 &ioNode

);

AUGraphOpen (processingGraph); // indirectly performs audio unit instantiation（间接执行）

// Obtain a reference to the newly-instantiated I/O unit。（从AUGraphNodeInfo中获取引用给ioUnit）

AudioUnit ioUnit;

AUGraphNodeInfo (

 processingGraph,

 ioNode,

 NULL,

 &ioUnit

);

{% endhighlight %}

上面介绍了AUNode，不透明类型，代表了音频单元的音频范围内处理图形。您会收到新的audio unit 实例，在ioUnit参数，对输出AUGraphNodeInfo函数调用。

第二个参数（ioNode）：定义了对audio unit 的描述

在获取了audio unit 实例后，you can configure it ，要做到这一点，你还需要了解 audio unit 的2个特点，范围和元素 

<h1>使用范围和元素以指定audio unit 部分</h1>

------------------------------------------------------------------------------------------------------------------------------

一个音频单元的部件被组织成的范围和内容，如图1-2所示。当调用功能配置或控制audio unit，可以指定范围和元素，以确定具体的目标函数


scope在audio unit内是编程context，虽然the name global scope 建议其他，这些情况下都绝不嵌套。你可以使用枚举

element 是在audio unit scope内，当element是input （或 output） scope的一部分，它类似于一个信号总线在物理音频设备，并且由于该原因是有时也称为总线(bus)

element 和bus 这两个词在audio unit 编程完全一样的东西。本文档在强调信号流时使用"bus",强调audio unit 具体功能方面时使用element，such the input and output elements of an I/O unit (see “Essential Characteristics of I/O Units” (page 18)).您可以通过它的零索引的整数值指定一个element（或bus）。如果设置一个属性或参数 
适用于整体范围，指定element value 为0。


上图展示出的是audio unit 通用的架构，其中element 的input 和output 数目一样，然而，各种音频设备用使用不同的架构。a mixer unit ，例如
可能有多个output elements ，但只有一个output elements。您可以扩展您学到什么这里大约范围和内容，以任何音频单元，尽管有这些变化的架构。


global scope ，在上图的底部，适用于audio unit 作为一个整体，是不与任何特定的音频流相关联。它有且只有一个元素，即 element 为0。一些属性，如
(kAudioUnitProperty_MaximumFramesPerSlice)，只作用于global scope

input 和output scopes 直接参与 移动一个或多个audio strems 通过 audio unit。如你所愿，audio输入 在iput scope 并且 输出在output scope。
属性或参数作为一个整体应用在input 或output scope 。elements 中 的count 属性 (kAudioUnitProperty_ElementCount) 就是这样.
例如，其他属性和参数，如I/O property (kAudioOutputUnitProperty_EnableIO) or the volume parameter(kMultiChannelMixerParam_Volume)
适用于指定元素的范围内


<h1>Use Properties to Configure Audio Units</h1>

------------------------------------------------------------------------------------------------------------------------------

 audio unit property 是使用key-value的形式来configure.kAudioUnitProperty_MaximumFramesPerSlice = 14,苹果公司保留属性键，从0到 

63999.在Mac OS X中，第三方音频设备使用超过这个范围的键。


每个属性的值是一个指定的数据类型，并有指定的读/写访问.具体的描述你可以在Audio Unit Properties Reference中找到.要设置audio unit 的属性，

使用函数 AudioUnitSetProperty.清单1-4显示了如何使用这个函数，指定的范围和元素以及指示键和值的属性。

Listing 1-4 Using scope and element when setting a property
{% highlight ruby %}
UInt32 busCount = 2;

OSStatus result = AudioUnitSetProperty (

 mixerUnit,

 kAudioUnitProperty_ElementCount // the property key

 kAudioUnitScope_Input, // the scope to set the property on

 0, // the element to set the property on

 &busCount, // the property value

 sizeof (busCount);
{% endhighlight %}



在这里，你会在音频单元开发经常使用的一些属性。熟悉每种通过阅读它的参考文档，并探讨苹果的音频单元示例代码的项目，如IOHost和调音台（MixerHost）：


1、kAudioOutputUnitProperty_EnableIO,启用或禁用 io ，默认情况下，output 是开启的，input是关闭的。

2、kAudioUnitProperty_ElementCount,配置混频器单元的输入元件的数目。

3、kAudioUnitProperty_MaximumFramesPerSlice,用于指定的帧的最大数目，音频数据的音频单元应准备，以产生响应渲染调用。对于大多数音频 

单位，在大多数情况下，你必须为参考文档中的说明设置此属性。如果您不这样做，你的声音将停止时，屏幕锁。

4、kAudioUnitProperty_StreamFormat，用于指定的音频流的数据格式用于特定音频单元的输入或输出总线。（用于指定的音频流的数据格式用于特定audio unit 的output 或input bus）


audio unit init的时候可以设置大多数属性值，这样的性质并不意由用户改变。不过，当设置iPoad EQ unit 的kAudioUnitProperty_PresentPreset 属性和Voice- Processiong I/O unit 的

kAUVoiceIOProperty_MuteOutput属性时，可以在音频播放的时候被改变


要发现一个属性的可用性，访问它的价值，并监视更改它的值，请使用以下功能：

1、AudioUnitGetPropertyInfo—用于查看一个属性是否可用;如果是，您将得到数据大小为它的价值，你是否可以更改该值。

2、AudioUnitGetProperty, AudioUnitSetProperty—To get or set the value of a property

3、AudioUnitAddPropertyListener, AudioUnitRemovePropertyListenerWithUserData ，要安装或移除的回调函数监视更改属性值


<h1>使用参数和UIKit给予用户控制</h1>

不同的属性值，每一个参数的值是相同的类型：32位floating.在允许范围值和度量单位，表示由audio unit 执行参数来确定,关于audio unit 参数方面的，可以查看Audio Unit Parameters Reference


获取或设置一个参数值时，请使用以下功能，在音频单元组件服务参考全面的描述之一：

■ AudioUnitGetParameter

■ AudioUnitSetParameter


以允许用户控制音频单元,通过接口的方式获得其参数。开始通过选择适当的classfrom UIKit框架来代表参数。例如，用于接通/

关断特性，如多通道混频器单元的kMultiChannelMixerParam_Enable参数，可以使用UISwitch对象。对于一个连续变化的功能，如立体声声像位置，如提供 

在kMultiChannelMixerParam_Pan参数，你可以使用一个UISlider对象。


<h1>I/O unit 基本特征</h1>

一个I/ O unit只包含两个元素，正如你在图1-3中看到的


虽然这两个elements 是一个audio unit 的一部分。您的应用程序将它们大多作为独立的实体。。例如，您使用的启用I / O属性（kAudioOutputUnitProperty_EnableIO），以启用或禁用单独的每个元素，根据您的应用程序的需求。

图中element 1 直接连接到音频输入的硬件设备上，。这种硬件连接的元件的输入范围1，是不透明的给你。你的第一个 
访问从输入硬件输入的音频数据是在元件1的输出范围

element 0 直接连接到音频输出的硬件设备上


input element  value 是1，output element value 是 0


正如你在图1-3中看到（第18页），每个元素本身具有的输入范围和输出范围。为了这因此，描述一个I / O单元的这些部分可能会有点混乱。例如，你可以说，在一个 

同时I / O的应用程序，您从输入元件的输出范围，接收音频和音频发送​​到输出元件的输入范围。当您需要，请返回到这个数字


最后，I / O单元能够启动和停止中的音频的音频流的唯一的音频单元处理图形。这样一来，在I/ O单元负责在音频单元应用程序中的音频流。


<h1>Audio Processing Graphs Manage Audio Units</h1>

音频处理图是核心基础式的不透明型，AUGraph，你用它来构建并管理音频单元的处理链。一个图可以利用多种音频设备的能力 和多渲染回调函数，使您可以创建几乎任何音频处理解决方案，您可以 

想象


该AUGraph型增加了线程安全的音频单元故事：它使您能够重新配置处理链上飞。例如，你可以安全地插入均衡器，甚至换了不同的渲染回调函数的混频器输入，而音频播放。事实上，AUGraph类型提供在IOS的唯一的API 
在音频应用程序进行这种动态重构。


音频处理图形API使用另一种不透明的类型，AUNode，代表一个单独的音频单元内的曲线图的情况下。当使用graph，你通常用节点作为代理来进行交互的包含的音频单元，而不是与该音频单元直接交互


当把一个graph连在一起，但是，您必须配置每个音频单元，要做到这一点，你必须直接与该音频单元由音频单元的API的方式进行交互。音频单元的节点，本身是不可配置的。通过这种方式，构建一个图，您需要使用的两种API，如“使用说明两个音频单元的API演唱会“（第13页）。


您也可以使用AUNode实例作为一个元素在一个复杂的图形定义的节点代表一个完整的audio processing subgraph。在这种情况下，在子结束时，I / O单元必须是一个通用 
输出单元的一种类型的I / O单元，不连接到设备硬件。


构建audio processing graph要完成3步

1、adding nodes to a graph

2、直接配置由节点所表示的audio unit 

3、互连节点


有关这些任务和在音频处理图形生命周期的其余部分的信息，请参阅“构建 音频单元应用程序“（第29页）。对于这一丰富的API的完整说明，请参见 Audio Unit Processing Graph Services Reference.


<h1>An Audio Processing Graph有且只有一个I/0 Unit</h1>

--------------------------------------------------------------------------------------------------------------------------------


Every audio processing graph has one I/O unit,无论你正在做的录音，回放，或同时 I / O。I/O unit 都可以完成，取决于你的需要。详细内容 
如何I / O单元适合于各种应用场景的音频处理图形架构，请参阅“启动 选择一个设计模式“（第29页）。


Graphs let you start and stop the flow of audio by way of the AUGraphStart and AUGraphStop functions.
These functions, in turn, convey the start or stop message to the I/O unit by invoking its
AudioOutputUnitStart or AudioOutputUnitStop function. In this way, a graph’s I/O unit is in charge
of the audio flow in the graph.


<h1>Audio Processing Graphs提供线程安全</h1>

---------------------------------------------------------------------------------------------------------------------------------

音频处理图形API采用了“待办事项列表”的比喻来提供线程安全。某些功能在此API的工作单元添加到更改的列表后执行。你后面指定了一套完整的变化， 然后你问图形加以实施


下面是由音频处理图形API支持的一些常见的重新配置，以及它们的相关功能：


■ Adding or removing audio unit nodes (AUGraphAddNode, AUGraphRemoveNode)

■ Adding or removing connections between nodes (AUGraphConnectNodeInput,
AUGraphDisconnectNodeInput)

■ Connecting a render callback function to an input bus of an audio unit
(AUGraphSetNodeInputCallback)



让我们来看看重新配置正在运行的音频处理图形的例子。举个例子，你已经建立了一个图，包括多通道混音器单元和远程I/ O单元，用于两个混合播放合成 

的声音。你喂的声音到混频器的两个输入总线。该混频器输出到输出单元 在I / O单元和到输出的音频硬件。图1-4描述了这种架构。



现在，假设用户希望将一个均衡到两个音频流中的一个。要做到这一点，添加了iPod的EQ的声音中的一个的进料和混频器输入之间的单元，它进入，如图1-5所示。


要做到这一点活重新配置的步骤如下：


1、从mixer unit 中的input 1断开“beats sound ”的回调，通过调用AUGraphDisconnectNodeInput.

2、添加包含了iPod的EQ单元图形的音频单元节点。通过与AudioComponentDescription结构指明了iPod的EQ单元，然后调用AUGraphAddNode做到这一点。在这一点上，在iPod EQ单元被实例化，但是没有初始化。它是由graph但尚未参与该音频流。

3、配置和初始化的iPod的EQ单元。在这个例子中，这需要几件事情：

	■调用AudioUnitGetProperty函数来获取流格式（kAudioUnitProperty_StreamFormat）从混频器的输入。

	■调用AudioUnitSetProperty功能两次，第一次以设置流格式在iPod的EQ unit'sinput和第二次以上的输出进行设置。 （有关如何配置一个完整的描述 
	一个iPod的EQ单元，请参阅“使用单位的影响”（第45页）。）

	■调用AudioUnitInitialize功能分配资源为iPod的EQ单位并准备以处理音频。这个函数调用是不是线程安全的，但你可以（而且必须）在这一点上执行它 
	的序列中，当在iPod EQ单元尚未积极参与音频处理中,graph中，因为你还没有叫AUGraphUpdate功能。

4、Attach the “beats sound” callback function to the input of the iPod EQ by calling AUGraphSetNodeInputCallback.


在上述列表中，步骤1,2和4个个AUGraph*函数调用，加入到该图表的 
“待办事项”列表中。呼叫AUGraphUpdate执行这些尚未完成的任务。在成功返回AUGraphUpdate的 
函数，图形已被动态重新配置和iPod的EQ到位和处理音频。


<h1>Audio Flows Through a Graph Using “Pull”</h1>

---------------------------------------------------------------------------------------------------------------------------------

In an audio processing graph,需要更多的音频数据.有一个流用于音频数据的请求，并且该流程前进的方向相反的音频流的。图1-6说明了这一机制。


一组数据的每个请求都被称为渲染调用，或非正式地，作为一拉。这个数字代表呈现 呼吁灰“控制流”箭头。通过渲染调用请求的数据更正确地称为一组音频采样帧（见核心音频术语“框架”）。

反过来，一组到渲染调用提供响应的音频采样帧被称为一个切片。 （请参阅“切片”核心音频词汇表。），提供片被称为渲染回调函数的代码，描述 “渲染回调函数饲料音频音频单元”（第23页）。


下面是如何pull proceeds，如图1-6：


1，当你调用AUGraphStart功能，虚拟输出设备调用的渲染回调远程I/ O单元的输出元素。这个调用请求处理后的音频数据帧中的一个切片。

2，使Remote I/ O unit 查看其输入缓冲器音频数据处理的回调函数，满足渲染调用。如果有数据等待处理，Remote I/ O unit使用它。否则， 
而如图中所示，它代替调用任何的渲染回调的程序已经连接到它的输入。在这个例子中，远程I/ O单元的输入连接到效果器的输出。所以，在 
I / O单元的拉动效果器上，要求音频帧的切片

3，effect unit的行为就像远程I/ O单元一样。当它需要的音频数据，它得到它从它的输入连接。在这个例子中，在效果器上的应用程序的渲染回调函数拉。

4，你的应用程序的渲染回调函数是拉的最终接收者。它提供所需的帧的效果器。

5，effect unit处理您的应用程序的渲染回调提供切片。effect unit则提供先前请求（步骤2），Remote I/ O unit处理的数据。

6,Remote I/ O unit 处理的effect unit提供的切片Remote I/ O unit元，然后用品经处理的切片最初请求（步骤1）到虚拟输出设备。这样就完成了一个循环拉。


<h1>Render Callback Functions Feed Audio to Audio Units</h1>

---------------------------------------------------------------------------------------------------------------------------------

提供从磁盘或存储器中的音频到音频单元输入总线，采用了渲染回调函数传递它符合该AURenderCallback原型。音频单元输入调用你的回调时，需要样品架的另一片，如“音频流过图使用”拉“”（第21页）。


写一个渲染回调函数的过程是设计和建造的。或许最有创意的方面音频设备的应用程序。这是你的机会，去创造或以任何方式，你可以想像改变声代码。


同时，呈现回调。您必须遵守严格的性能要求。渲染的实时优先级的线程在其后续的渲染调用异步到达回调的生活。您在呈现回调的身体做好了发生在这段时间受限的环境。如果您的回调 
仍然是生产样本帧响应于先前的渲染调用时的下一个渲染呼叫到达时，将得到的声音间隙。出于这个原因，你不能使用锁，分配内存，存取文件系统或网络连接，或以其他方式的呈现回调函数体执行耗时的任务。


<h1>了解音频单元渲染回调函数</h1>

清单1-5显示了一个渲染回调函数符合AURenderCallback原型的头。 本节将介绍它的每个参数的目的又和说明如何使用每一个。


Listing 1-5 A render callback function header


{% highlight ruby %}

static OSStatus MyAURenderCallback (

 void *inRefCon,

 AudioUnitRenderActionFlags *ioActionFlags,

 const AudioTimeStamp *inTimeStamp,

 UInt32 inBusNumber,

 UInt32 inNumberFrames,

 AudioBufferList *ioData

) { /* callback body */ }

{% endhighlight %}
该inRefCon参数指向一个纲领性的情况下，你安装回调到时指定音频输入单元（见“写和附加渲染回调函数”（第36页））。的这方面的目的是提供回调函数，它需要计算的任何音频输入数据或状态信息 
音频输出为给定的渲染调用


该ioActionFlags参数允许的回调提供了一个暗示，音频单元，没有音频 
流程。做到这一点，例如，如果你的应用程序是一种人工合成的吉他和用户是不是正在播放一个音符。 
在您想要输出的沉默回调调用，使用像在下面的声明 
body的回调：

*ioActionFlags |= kAudioUnitRenderAction_OutputIsSilence;


当你想制作的沉默，你也必须显式地设置缓冲区指向的ioData参数0有更多有关这方面的描述，该参数


该inTimeStamp参数表示在该回调被调用的时间。它包含一个 AudioTimeStamp结构，其mSampleTime字段是一个采样帧计数器。在每次调用 
回调的mSampleTime字段增量通过在inNumberFrames数的值 参数。如果你的应用程序是一个音序器或鼓机，例如，您可以使用mSampleTime值 
调度的声音。


该inBusNumber参数指示调用回调的音频单元巴士，让您分支回调内根据此值。此外，附加回调到音频设备时，就可以指定一个不同的上下文（inRefCon）每个总线


该inNumberFrames参数指示音频采样的帧数回调正在要求提供关于当前调用。您提供的帧缓冲区中的ioData参数


该ioData参数指向音频数据缓冲区的回调必须填写被调用时。该音频放置到这些缓冲区必须符合调用该总线的音频流格式回调


如果你是玩沉默回调特定的调用，显式地设置这些缓冲区为0，如通过使用memset的函数


图1-7描述了对非交叉立体缓冲区的ioData参数。使用的元素图中的可视化的回调需要填写ioData缓冲器的细节。


<h1>音频流格式启用数据流量</h1>


当音频数据在各个采样级别的工作，当你使用音频设备时，这是不够的指定代表音频正确的数据类型。这些位在一个单一的音频采样值的布局具有意义，所以像浮点32或UINT16数据类型不是表现不够。在本节中，您了解核心音频的解决了这个问题。


<h3>Working with the AudioStreamBasicDescription structure</h3>

---------------------------------------------------------------------------------------------------------------------------------

该货币对在你的应用程序四处移动音频值，和你的应用程序和音频硬件之间，是AudioStreamBasicDescription结构，如清单1-6所示，并在核心AudioData类型参考充分说明。

Listing 1-6 The AudioStreamBasicDescription structure

{% highlight ruby %}
struct AudioStreamBasicDescription {

 Float64 mSampleRate;

 UInt32 mFormatID;

 UInt32 mFormatFlags;

 UInt32 mBytesPerPacket;

 UInt32 mFramesPerPacket;

 UInt32 mBytesPerFrame;

 UInt32 mChannelsPerFrame;

 UInt32 mBitsPerChannel;

 UInt32 mReserved;
};
typedef struct AudioStreamBasicDescription AudioStreamBasicDescription;

{% endhighlight %}
因为名称AudioStreamBasicDescription长，它通常缩写的谈话，文档为ASBD。要为ASBD的字段定义的值，写类似的代码清单所示 1-7。


Listing 1-7 Defining an ASBD for a stereo stream
{% highlight ruby %}
size_t bytesPerSample = sizeof (AudioUnitSampleType);

AudioStreamBasicDescription stereoStreamFormat = {0};

stereoStreamFormat.mFormatID = kAudioFormatLinearPCM;

stereoStreamFormat.mFormatFlags = kAudioFormatFlagsAudioUnitCanonical;

stereoStreamFormat.mBytesPerPacket = bytesPerSample;

stereoStreamFormat.mBytesPerFrame = bytesPerSample;

stereoStreamFormat.mFramesPerPacket = 1;

stereoStreamFormat.mBitsPerChannel = 8 * bytesPerSample;

stereoStreamFormat.mChannelsPerFrame = 2; // 2 indicates stereo

stereoStreamFormat.mSampleRate = graphSampleRate;
{% endhighlight %}


开始，确定表示一个音频采样值的数据类型。本示例使用 AudioUnitSampleType定义的类型，对于大多数音频设备推荐数据类型。在iOS上， 
AudioUnitSampleType被定义为一个8.24定点整数。清单1-7计算在所述第一线 在该类型的字节数;限定一些的ASBD的字段值时，需要该数目，你可以在列表中看到。
接下来，仍参照清单1-7，声明类型AudioStreamBasicDescription的变量并初始化其字段为0，以确保没有字段包含垃圾数据。不要跳过这一步归零;如果你这样做，你是 
一定会遇到麻烦


现在定义ASBD字段值。指定kAudioFormatLinearPCM的mFormatID领域。音频设备使用未压缩音频数据，所以这是正确的格式标识符，只要您使用音频使用单位。


其次，对于大多数音频单元，指定kAudioFormatFlagsAudioUnitCanonical metaflag为mFormatFlags场。这个标志在CoreAudio.framework/ CoreAudioTypes.h定义如下：
{% highlight ruby %}
kAudioFormatFlagsAudioUnitCanonical = kAudioFormatFlagIsFloat |

 kAudioFormatFlagsNativeEndian |

 kAudioFormatFlagIsPacked |

 kAudioFormatFlagIsNonInterleaved
{% endhighlight %}

个元标记需要在指定类型的线性PCM采样值的所有的布局细节位的护理AudioUnitSampleType。


某些音频单元采用了非典型的音频数据格式，需要的样本和不同的数据类型 不同的标志的mFormatFlags领域。例如，在 3D Mixer unit r需要UINT16数据 
键入其音频的采样值，并且需要ASBD的mFormatFlags字段被设置为 
kAudioFormatFlagsCanonical。当与特定的音频单元的工作，要小心使用正确的 数据格式及格式是否正确的标志。 （请参阅“使用特定的音频单元”（第41页）。）


通过上市1-7（第25页）继续进行，在接下来的四个字段进一步明确组织和意义在样本帧的位。设置这些字段，mBytesPerPacket，mBytesPerFrame，mFramesPerPacket， 
和mBitsPerChannel领域，根据您所使用的音频流的性质。学习每个字段的含义，请参阅文档的AudioStreamBasicDescription 结构。你可以看到在示例代码项目，调音台（MixerHost），并填写了ASBDs的例子 
Mixer3DHost


根据流1通道单声道的数量将ASBD的mChannelsPerFrame场音频，立体声为2，依此类推。


最后，根据你是在你的应用程序中使用的采样率设置mSampleRate领域。“了解在哪里，以及如何设置流格式”（第26页）解释避免的重要性 而不是由字段中指定的ASBD场，你在这里看到的，您可以使用提供的c + +的实用方法在CAStreamBasicDescription.h文件（/开发/其他/ CoreAudio的/ PublicUtility/）。在特别是，查看SetAUCanonical和SetCanonical c + +的方法。这些指定的正确方法 
获得给定的三个因素ASBD字段的值：采样率转换。 “配置您的音频会话”（第34页）说明如何确保您的应用程序的采样速率相匹配的音频硬件采样率。


■无论是流是用于I / O（SetCanonical）或用于音频处理（SetAUCanonical） 

■如何多渠道想流格式来表示 

■无论你是想流格式交错或非交叉



无论您是否包括CAStreamBasicDescription.h文件在你的项目中使用它的方法 直接，Apple建议你学习该文件的学习与工作的正确方法 AudioStreamBasicDescription结构。


关于如何解决相关的音频数据流格式问题的想法，请参见“故障排除提示”（第39页）。


<h1>了解在哪里以及如何设置流格式</h1>


则必须设置在临界点的音频数据流的格式中的音频处理图。在其他地方，系统设置的格式。在另一些点，音频单元连接传播，从一个流格式音频单元到另一个


音频输入和输出硬件iOS设备上具有系统确定的音频流格式。这些格式始终是未压缩的，以线性PCM格式，交错。该系统规定这些格式上朝外的I / O单元的两侧中的音频处理图形，如在图1-8中描绘。



在该图中，麦克风表示输入音频的硬件。该系统确定该输入 硬件的音频流格式，并规定它放到Remote I/O unit的输入元件的输入范围。


类似地，在图中的扬声器代表输出的音频硬件。该系统确定的输出硬件的数据流格式，并规定它放到Remote I/O unit的输出的输出范围元素。


您的应用程序负责建立音频流格式上的向内的侧面 I / O unit的元素。在I/ O单元的应用程序格式之间进行的任何必要转换 硬件格式。你的应用程序还负责制定流格式任何其他地方都 
在图表中必需的。在某些情况下，在多通道混频器单元的如图1-8的输出，例如，你需要设置的格式，即，采样率的仅一部分。通过选择设计“开始模式“（第29页）显示您在哪里设置流格式的不同类型的音频设备的应用程序。 “使用特定的 音频单元“（第41页）中列出的数据流格式要求每个iOS的音频单元。


音频设备连接的一个关键特性，如图1-8（第27页），是连接传播从它的源的音频单元的输出的音频数据流的格式的其目的地的音频输入单元。这是一个关键点，以便需要强调的：流格式传输发生了的方式 
音频单元连接并仅在一个方向 - 从源音频单元的输出来的输入目标音频单元。


利用格式传播。它可以显著减少代码需要编写量。对于不需要设置流格式的I / O单元。它是由间的连接适当地设定音频单元，基于所述混频器的输出数据流的格式（参见图1-8（第27页））。


流格式传播发生在一个特定的点在音频处理图形的生活周期，即在初始化时。请参阅“初始化并启动音频处理图形”（第38页）。


使用该硬件使用的采样率。当你这样做时，I/ O单元不需要进行采样率转换。这在移动设备中，并最大限度地减少能量使用，一个重要的考虑音频质量。要了解有关与硬件采样率工作，请参阅“配置您的音频会议”（第 
34）。


<h1>Constructing Audio Unit Apps</h1>

---------------------------------------------------------------------------------------------------------------------------------


现在你明白了音频单元主机的作品，如“音频单元解释托管基本原理“（第11页），你有充分的准备来构建您的应用程序的音频单元部分。的主要步骤在选择一种设计模式，然后编写代码来实现这个模式
首先，选择一个设计模式


半打的基本设计模式举办的音频设备在iOS应用程序。通过选取一个开始最能代表你希望你的应用做了什么声音。当你学习的每个图案时，注意共同的特点。每一个模式：

■有且只有一个I / O单元。


■使用单一的音频流格式在整个音频处理图形，虽然可以有变型在该格式中，如单声道和立体声流供给混频器单元
■需要设置的流格式，或者流格式的部分，在特定位置


设置流格式正确是必不可少的建立音频数据流。大多数这些模式的依靠音频流格式从源到目的地的自动传播，通过音频单元所提供 连接。借此传播优势的时候可以，因为它减少了代码量 编写和维护。同时，要确保你知道它需要你设置流 格式。例如，你必须设置一个iPod的EQ单元的输入和输出的全码流格式。请参阅为所有的iOS设备的音频流格式要求“使用特定的音频单元”（第41页）中使用的表。


在大多数情况下，在本章的设计模式使用的音频处理图形（类型AUGraph的）。您 可以实现这些模式中的任何一个没有使用的曲线图，但使用一个简化的代码和 
支持动态重配置，如“Audio Processing Graphs Manage Audio Units”（第页 19）。


<h1>I/O Pass Through(通过)</h1>

---------------------------------------------------------------------------------------------------------------------------------


在I / O直通模式直接输出的硬件将输入的音频，没有选择工作与该音频数据。虽然这不是太大的实用价值，构建托管的应用程序的音频单元基于在这个模式是一个很好的方式来验证和巩固你的音频单元概念的理解。图2-1 
示出了这种模式。


正如你在图中看到，音频输入硬件强加的朝外一侧的流格式 远程I / O单元的输入元素。你，反过来，指定要在使用的格式 内向该元素的面。根据需要，该音频装置进行格式转换。以避免不必要的 
采样率转换，务必确定您的流格式时使用的音频硬件采样率。


input元素默认是禁用的，所以一定要启用它;否则，音频不能获取到流


在图2-1所示的模式需要两个远程I之间的音频设备连接的优势/ O 元素。特别是，您没有设置对音频设备的输出元件的输入范围流格式。连接传播你的输入元件中指定的格式。


输出元件的朝外一侧发生在音频输出硬件的流格式，并根据需要输出元件执行的格式转换为输出音频。 


使用这个模式，你不需要配置任何音频数据缓冲区。










